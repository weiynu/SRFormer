import argparse
import numpy as np
import pandas as pd
import os
from os.path import dirname
import torch
import torch.nn as nn
import yaml
import sys
from torch.optim import Adam


root=dirname(os.getcwd())+"/lib/" #pycharm环境
sys.path.append(root)
sys.path.append("..")

from STAEformer import STAEformer
from lib.data_prepare import get_dataloaders_from_index_data
from train import RMSE_MAE_MAPE
from wbcp import wbcp_inner_optimize

# -----------------------
# load pretrained STAEformer as the target of refinement
# -----------------------
def load_model(model_path, device):
    model_name = STAEformer.__name__
    with open(f"./{model_name}.yaml", "r") as f:
        cfg = yaml.safe_load(f)
    cfg = cfg['PEMS08']
    model = STAEformer(**cfg["model_args"])
    state_dict = torch.load(model_path, weights_only=True)
    model.load_state_dict(state_dict)
    return model.to(device)

# -----------------------
# KL divergence KL(p || q) where p = alpha_tilde, q = alpha_model
# compute the similarity between attention map and their supervision per-batch mean
# -----------------------
def kl_divergence(p, q, eps=1e-8):
    # p, q: (B,S), both prob distributions (sum to 1)
    p = p.clamp(min=eps)
    q = q.clamp(min=eps)
    return (p * (torch.log(p) - torch.log(q))).sum(dim=1).mean()


# -----------------------
# refine the attention scores generated by the pretrained model
# -----------------------
def attention_refinement(net1, net2, model, dataloader, device):
    '''
    1. net1 and net2 are the output_proj module in STEAformer, used to generate predictions.
    2. model is used to refine the attention weights.
    '''

    model.eval()
    optimizer = Adam(model.parameters(), lr=1e-4)
    criterion = nn.CrossEntropyLoss()

    for x, y in dataloader:
        x = x.to(device)
        y = y.to(device)

        # generate embeddings for subsequent wbcp computation
        with torch.no_grad():
            pred, att, embeddings = model(x)

        sigma, att_tilde = wbcp_inner_optimize(
            embeddings,
            net1,
            net2,
            inner_steps=30,
            lr_inner=0.02,
            rate1=0.1,
            rate2=1.0,
            device=device)

        # unfreeze model params, compute classification + attention KL ----------
        for p in model.parameters():
            p.requires_grad = True
        model.train()
        optimizer.zero_grad()

        pred, att_model, embeddings = model(x)  # forward normally
        clf_loss = criterion(pred, y)
        # compute KL(alpha_tilde || alpha_model)
        # kl = kl_divergence(att_tilde,att_model.detach())  # detach alpha_model? No: we should allow gradients to flow into model. Do not detach alpha_model.
        # NOTE: keep α_model differentiable. So compute without detach:
        kl = kl_divergence(att_tilde, torch.mean(att_model,dim=-1).reshape(att_tilde.shape))
        gamma = 1.0  # weight of attention supervision
        loss = clf_loss + gamma * kl

        loss.backward()
        optimizer.step()

        # logging
        print(f"clf_loss={clf_loss.item():.4f} kl={kl.item():.4f}")

    print("Refinement finished.")

# -----------------------
# the entrance of refinement
# -----------------------
if __name__ == '__main__':
    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')
    data_path = f"../data/PEMS08"
    model_name = STAEformer.__name__
    model_path = f"../saved_models/STAEformer-PEMS08-2025-10-27-21-58-25.pt"

    with open(f"./{model_name}.yaml", "r") as f:
        cfg = yaml.safe_load(f)
    cfg = cfg['PEMS08']

    trainset_loader, valset_loader, testset_loader, SCALER, = get_dataloaders_from_index_data(
        data_path,
        tod=cfg.get("time_of_day"),
        dow=cfg.get("day_of_week"),
        batch_size=cfg.get("batch_size", 64),
    )

    model=load_model(model_path, device)

    in_steps = 12
    model_dim = 152
    out_steps = 12
    output_dim = 1
    predictor1 = nn.Linear(in_steps * model_dim, out_steps * output_dim)
    predictor2 = nn.Linear(in_steps * model_dim, out_steps * output_dim)

    attention_refinement(predictor1, predictor2, model, testset_loader, device)